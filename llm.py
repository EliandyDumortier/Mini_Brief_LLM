#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import shutil
import chromadb
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_chroma import Chroma
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import gradio as gr

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PDF_DIR        = "data"                      # dossier contenant Menu.pdf et Liste_allergenes.pdf
CHROMA_DB_DIR  = "./chroma_db"
COLLECTION     = "bella_napoli_pizza"
MODEL_NAME     = "mistral:latest"
os.environ["CHROMA_ENABLE_TELEMETRY"] = "false"

# â”€â”€ (Re)crÃ©ation du dossier ChromaDB pour Ã©viter les collisions d'embed dims â”€
if os.path.exists(CHROMA_DB_DIR):
    shutil.rmtree(CHROMA_DB_DIR)

# â”€â”€ 1) Chargement des PDF avec metadata Â« source Â» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
menu_loader      = PyPDFLoader(os.path.join(PDF_DIR, "Menu.pdf"))
allergy_loader   = PyPDFLoader(os.path.join(PDF_DIR, "Liste_allergenes.pdf"))

docs_menu    = menu_loader.load()    # liste de Document(page_content, metadata)
docs_allergy = allergy_loader.load()

# On tagge chaque document avec la source pour pouvoir citer si besoin
for doc in docs_menu:
    doc.metadata["source"] = "menu"

for doc in docs_allergy:
    doc.metadata["source"] = "allergenes"

# Combine les deux jeux de docs
docs = docs_menu + docs_allergy

# â”€â”€ 2) DÃ©coupage en chunks pour la recherche â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
chunks   = splitter.split_documents(docs)

# â”€â”€ 3) Index vectoriel ChromaDB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client     = chromadb.PersistentClient(path=CHROMA_DB_DIR)
embeddings = OllamaEmbeddings(model=MODEL_NAME)

vectordb = Chroma.from_documents(
    chunks,
    embedding=embeddings,
    client=client,
    collection_name=COLLECTION
)

# â”€â”€ 4) PromptTemplate intelligent â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
template = """
Vous Ãªtes lâ€™assistant de Bella Napoli.
Vous disposez de deux sources :
- le MENU (source="menu") contenant les noms de pizzas et leurs ingrÃ©dients,
- la LISTE_ALLERGENES (source="allergenes") listant chaque allergÃ¨ne et son numÃ©ro.

1. Si la question porte sur les **ingrÃ©dients** dâ€™une pizza X, rÃ©pondez :
   â€œIngrÃ©dients de la pizza X : â€¦.â€

2. Si la question porte sur les **allergÃ¨nes** dâ€™une pizza X, rÃ©pondez :
   â€œAllergÃ¨nes de la pizza X (codes) : â€¦.â€
   Si possible, donnez aussi le nom de lâ€™allergÃ¨ne entre parenthÃ¨ses.

3. Si la pizza X nâ€™existe pas dans le MENU, rÃ©pondez :
   â€œDâ€™aprÃ¨s la documentation Bella Napoli, il nâ€™existe pas de pizza Â« X Â» dans le menu fourni.â€

4. Nâ€™ajoutez pas dâ€™informations non prÃ©sentes dans les documents.

Contexte (extraits les plus pertinents) :
{context}

Question :
{question}

RÃ©ponse :
"""
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=template.strip()
)

# â”€â”€ 5) Construction de la chaÃ®ne RAG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
retriever = vectordb.as_retriever(search_kwargs={"k": 4})
llm       = ChatOllama(model=MODEL_NAME)
qa_chain  = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=False
)

# â”€â”€ 6) Fonction dâ€™interrogation Gradio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def answer_question(full_question: str) -> str:
    # On extrait le nom de la pizza
    m = re.search(r"pizza\s+(.+?)[\?\.]?$", full_question, re.IGNORECASE)
    pizza_name = m.group(1).strip() if m else full_question.strip()
    # On construit la question pour le LLM
    query = full_question.replace(m.group(1), pizza_name) if m else full_question
    # On interroge la RAG
    result = qa_chain.invoke({"query": query})
    return result["result"]

# â”€â”€ 7) Lancement Gradio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
iface = gr.Interface(
    fn=answer_question,
    inputs=gr.Textbox(lines=2, placeholder="Ex : Quels ingrÃ©dients contient la pizza Margherita ?"),
    outputs="text",
    title="ğŸ• Assistant Bella Napoli",
    description="Posez vos questions sur les ingrÃ©dients ou les allergÃ¨nes des pizzas."
)

if __name__ == "__main__":
    iface.launch(share=True)
