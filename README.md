# ğŸ• Bella Napoli AI Assistant

Un assistant IA simple basÃ© sur une architecture RAG (Retrieval-Augmented Generation) et LangChain, pour rÃ©pondre aux questions sur les pizzas de la pizzeria **Bella Napoli** : ingrÃ©dients, allergÃ¨nes, etc. Lâ€™interface interactive est servie via Gradio et le LLM Mistral tourne localement via Ollama.

---

## ğŸ“‚ Structure du projet

```
Mini_Brief_LLM/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ Menu.pdf
â”‚ â””â”€â”€ Liste_allergenes.pdf
â”œâ”€â”€ chroma_db/ # (auto-gÃ©nÃ©rÃ©) base ChromaDB persistante
â”œâ”€â”€ llm.py # script principal
â”œâ”€â”€ requirements.txt # dÃ©pendances Python
â””â”€â”€ README.md # ce fichier

```
---

## âš™ï¸ PrÃ©requis

1. **Python â‰¥ 3.8**  
2. **Ollama** installÃ© et configurÃ© :  
   ```bash
   brew install ollama      # ou selon votre OS
   ollama pull mistral:latest
   ollama pull mxbai-embed-large:latest

    Un virtualenv (recommandÃ©) :

    python -m venv .venv
    source .venv/bin/activate

ğŸ“¥ Installation

    Clonez ce dÃ©pÃ´t et entrez dans le dossier :

git clone <votre-repo-url>
cd Mini_Brief_LLM

Installez les dÃ©pendances Python :

    pip install --upgrade pip
    pip install -r requirements.txt

ğŸ› ï¸ Configuration

    Dossier data/
    Placez vos fichiers PDF :

        Menu.pdf : liste des pizzas et leurs ingrÃ©dients.

        Liste_allergenes.pdf : codes et libellÃ©s des allergÃ¨nes.

    Variables dâ€™environnement
    (optionnels, si Ollama tourne sur un host diffÃ©rent)

export OLLAMA_HOST="http://localhost:11434"
export CHROMA_ENABLE_TELEMETRY="false"

ParamÃ¨tres dans llm.py
Vous pouvez modifier :

    PDF_DIR       = "data"
    CHROMA_DB_DIR = "./chroma_db"
    COLLECTION    = "bella_napoli_pizza"
    MODEL_NAME    = "mistral:latest"            # ou un autre modÃ¨le Ollama

ğŸš€ Lancement

    Assurez-vous dâ€™avoir pullÃ© les modÃ¨les Ollama :

ollama list
# â†’ doit afficher "mistral:latest" et "mxbai-embed-large:latest"

Lancez lâ€™application :

    python llm.py

    Ouvrez lâ€™URL affichÃ©e (par dÃ©faut http://127.0.0.1:7860) ou utilisez le lien public si share=True est activÃ©.

ğŸ¯ Utilisation

    Questions ingrÃ©dients

        Quels ingrÃ©dients contient la pizza Margherita ?
        â†’ liste des ingrÃ©dients.

    Questions allergÃ¨nes

        Quels sont les allergÃ¨nes de la pizza Reine ?
        â†’ codes et noms des allergÃ¨nes correspondants.

    Pizza inconnue

        Quels ingrÃ©dients contient la pizza Truffe dâ€™hiver ?
        â†’ message :

    Dâ€™aprÃ¨s la documentation Bella Napoli, il nâ€™existe pas de pizza Â« Truffe dâ€™hiver Â» dans le menu fourni.

ğŸ“¦ DÃ©pendances principales

langchain
langchain-ollama
langchain-chroma
chromadb
gradio

ğŸ”„ RecrÃ©ation de la base vectorielle

Si vous changez de modÃ¨le dâ€™embeddings (dimension diffÃ©rente), supprimez le dossier chroma_db/ avant de relancer :

rm -rf chroma_db/
python llm.py

ğŸ”§ Personnalisation

    Changer le modÃ¨le LLM : modifiez MODEL_NAME pour un autre tag Ollama.

    Ajuster k (nombre de documents RAG) : dans la crÃ©ation du retriever :

    retriever = vectordb.as_retriever(search_kwargs={"k": 4})

    PromptTemplate : dans llm.py, Ã©ditez template pour guider le style de rÃ©ponse.

ğŸ“ Licence

Projet libre, Ã  adapter et rÃ©utiliser selon vos besoins pÃ©dagogiques ou professionnels.